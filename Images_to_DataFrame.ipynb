{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Please download the dataset from the following link: \n",
    "\n",
    "# https://archive.ics.uci.edu/ml/datasets/Devanagari+Handwritten+Character+Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# You can download the csv files of the training as well as cross validation dataset from the following links:\n",
    "\n",
    "# https://drive.google.com/file/d/1LO7l5LqiGgRg_EUJhrwtrw6TRGeixahc/view?usp=sharing\n",
    "\n",
    "# https://drive.google.com/file/d/16KYVxtexm1Jzw_d7sv9ql3S33571U_J8/view?usp=sharingv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import numpy as np \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report,accuracy_score,precision_score,recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "class img_to_df:\n",
    "    \n",
    "    def __init__(self,path,train_cv_split):\n",
    "        \n",
    "        self.path = path\n",
    "        \n",
    "        self.unique_labels = os.listdir(path)\n",
    "        \n",
    "        self.train_cv_split = train_cv_split\n",
    "        \n",
    "        \n",
    "        \n",
    "    def list_of_images(self,folder):\n",
    "        \n",
    "        return os.listdir(os.path.join(self.path,folder))\n",
    "        \n",
    "        \n",
    "    \n",
    "    def read_image(self,folder,image):\n",
    "        \n",
    "        folder_path = os.path.join(self.path,folder)\n",
    "        \n",
    "        image_path = os.path.join(folder_path,image)\n",
    "        \n",
    "        image = plt.imread(image_path)\n",
    "        \n",
    "        return image.reshape(image.shape[0]*image.shape[1],)\n",
    "    \n",
    "    \n",
    "    \n",
    "    def stacking_row_vectors(self,folder):\n",
    "        \n",
    "        images_list = self.list_of_images(folder)\n",
    "        \n",
    "        images = list()\n",
    "        \n",
    "        for img in images_list:\n",
    "            \n",
    "            images.append(self.read_image(folder,img))\n",
    "            \n",
    "        train_len = int(self.train_cv_split[0]*len(images))\n",
    "            \n",
    "        return np.array(images[0:train_len]), np.array(images[train_len:])\n",
    "    \n",
    "    \n",
    "    \n",
    "    def generate_df(self):\n",
    "        \n",
    "        train_data = list()\n",
    "        \n",
    "        cv_data = list()\n",
    "        \n",
    "        for folder in self.unique_labels:\n",
    "            \n",
    "            train_folder_matrix, cv_folder_matrix = self.stacking_row_vectors(folder)\n",
    "            \n",
    "            train_data.append(train_folder_matrix)\n",
    "            \n",
    "            cv_data.append(cv_folder_matrix)\n",
    "            \n",
    "        train_data = np.concatenate(train_data,axis=0)\n",
    "        \n",
    "        cv_data = np.concatenate(cv_data,axis=0)\n",
    "        \n",
    "        train_labels = list()\n",
    "        \n",
    "        cv_labels = list()\n",
    "        \n",
    "        for folder_name in self.unique_labels:\n",
    "            \n",
    "            train_labels = train_labels + [folder_name]*train_folder_matrix.shape[0]\n",
    "            \n",
    "            cv_labels = cv_labels + [folder_name]*cv_folder_matrix.shape[0]\n",
    "            \n",
    "        train_data = pd.DataFrame(data=train_data)\n",
    "        \n",
    "        train_data['label'] = train_labels\n",
    "        \n",
    "        cv_data = pd.DataFrame(data=cv_data)\n",
    "        \n",
    "        cv_data['label'] = cv_labels\n",
    "        \n",
    "        return train_data,cv_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj = img_to_df(\"./DevanagariHandwrittenCharacterDataset/DevanagariHandwrittenCharacterDataset/Train\",(0.8,0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data, cv_data = obj.generate_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data.to_csv(\"./Devnagari_Handwritten_Character_Train.csv\")\n",
    "\n",
    "cv_data.to_csv(\"./Devnagari_Handwritten_Character_cv.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = pd.read_csv(\"Devnagari_Handwritten_Character_Train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data.drop([training_data.columns[0]],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>1015</th>\n",
       "      <th>1016</th>\n",
       "      <th>1017</th>\n",
       "      <th>1018</th>\n",
       "      <th>1019</th>\n",
       "      <th>1020</th>\n",
       "      <th>1021</th>\n",
       "      <th>1022</th>\n",
       "      <th>1023</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>character_10_yna</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>character_10_yna</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>character_10_yna</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>character_10_yna</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>character_10_yna</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1025 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     0    1    2    3    4    5    6    7    8    9  ...  1015  1016  1017  \\\n",
       "0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   0.0   \n",
       "1  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   0.0   \n",
       "2  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   0.0   \n",
       "3  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   0.0   \n",
       "4  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   0.0   \n",
       "\n",
       "   1018  1019  1020  1021  1022  1023             label  \n",
       "0   0.0   0.0   0.0   0.0   0.0   0.0  character_10_yna  \n",
       "1   0.0   0.0   0.0   0.0   0.0   0.0  character_10_yna  \n",
       "2   0.0   0.0   0.0   0.0   0.0   0.0  character_10_yna  \n",
       "3   0.0   0.0   0.0   0.0   0.0   0.0  character_10_yna  \n",
       "4   0.0   0.0   0.0   0.0   0.0   0.0  character_10_yna  \n",
       "\n",
       "[5 rows x 1025 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_data = pd.read_csv(\"Devnagari_Handwritten_Character_cv.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianNB:\n",
    "    \n",
    "    \"\"\"Instantiate a Gaussian Naive Bayes Object with the following parameters: \n",
    "        \n",
    "        features :               A dataframe consisting of continuous features, excluding labels\n",
    "        labels :                 A series consisting of binary labels\n",
    "        train_cv_test_split :    A tuple consisting of fraction for training, cross validation and testing data\n",
    "        apply_pca :              Boolean value specifying whether to apply PCA or not\n",
    "        n_principal_components : Number of Principal Components (Eigen vectors having non zero values to keep) \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,features,labels,train_cv_test_split,apply_pca,n_principal_components):\n",
    "        \n",
    "        self.unique_labels = list(labels.unique())\n",
    "        \n",
    "        self.labels = np.array(labels).reshape(labels.shape[0],1)\n",
    "        \n",
    "        self.train_cv_test_split = train_cv_test_split\n",
    "        \n",
    "        self.n_principal_components = n_principal_components\n",
    "        \n",
    "        if apply_pca == True:\n",
    "            \n",
    "            self.X_new = self.apply_dim_reduction(features,self.n_principal_components)\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "    def apply_dim_reduction(self,data,n_components):\n",
    "        \n",
    "        X = np.array(data)\n",
    "        \n",
    "        X_dash = X - np.mean(X,axis=0).reshape(-1,X.shape[1])\n",
    "        \n",
    "        sigma_hat = (1/data.shape[0])*np.matmul(X_dash.T,X_dash)\n",
    "        \n",
    "        sigma_hat_decompose = np.linalg.svd(sigma_hat)\n",
    "        \n",
    "        Q = sigma_hat_decompose[0]\n",
    "        \n",
    "        self.Q_tilda = Q[:,0:n_components]\n",
    "        \n",
    "        X_new = np.matmul(X_dash,self.Q_tilda)\n",
    "        \n",
    "        return X_new\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    def fit(self,data,alpha,gamma):\n",
    "        \n",
    "        self.likelihood_params = dict()\n",
    "        \n",
    "        sigma_hats = 0\n",
    "        \n",
    "        for label in self.unique_labels:\n",
    "        \n",
    "            mu_hat = np.array(data[data['label'] == label].iloc[:,0:self.n_principal_components].mean())\n",
    "\n",
    "            sigma_hat = np.array(data[data['label'] == label].iloc[:,0:self.n_principal_components].cov())\n",
    "            \n",
    "            sigma_hats = sigma_hats + 1359*sigma_hat\n",
    "            \n",
    "            self.likelihood_params[label] = [mu_hat,sigma_hat]\n",
    "            \n",
    "        self.sigma_hat = sigma_hats/(data.shape[0] - len(self.unique_labels))\n",
    "        \n",
    "        self.mean_variance = np.mean(np.diag(self.sigma_hat))\n",
    "            \n",
    "        for label in self.unique_labels:\n",
    "            \n",
    "            self.likelihood_params[label][1] = alpha*self.likelihood_params[label][1] + (1-alpha)*self.sigma_hat\n",
    "            \n",
    "            self.likelihood_params[label][1] = (1-gamma)*self.likelihood_params[label][1] + gamma*self.mean_variance*np.eye(self.n_principal_components,self.n_principal_components)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    def evaluate(self,data):\n",
    "        \n",
    "        inputs = np.array(data.iloc[:,0:self.n_principal_components])\n",
    "        \n",
    "        posterior = list()\n",
    "        \n",
    "        for label in self.unique_labels:\n",
    "    \n",
    "            posterior.append(s.multivariate_normal.pdf(inputs,self.likelihood_params[label][0],self.likelihood_params[label][1]).reshape(inputs.shape[0],1))\n",
    "        \n",
    "        posterior = np.concatenate(posterior,axis=1)\n",
    "        \n",
    "        predicted_category = pd.Series(np.argmax(posterior,axis=1))\n",
    "    \n",
    "        predicted_category.replace(to_replace=np.arange(0,len(self.unique_labels)),value=self.unique_labels,inplace=True)\n",
    "    \n",
    "        predicted_results = np.array(predicted_category)\n",
    "        \n",
    "        actual_results = np.array(data['label'])\n",
    "        \n",
    "        acc = accuracy_score(y_true=actual_results,y_pred=predicted_results)\n",
    "        \n",
    "        recall = recall_score(y_true=actual_results,y_pred=predicted_results,average='weighted')\n",
    "        \n",
    "        precision = precision_score(y_true=actual_results,y_pred=predicted_results,average='weighted')\n",
    "        \n",
    "        return {\"acc\":acc,\"recall\":recall,\"precision\":precision}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = dict()\n",
    "\n",
    "for n_components in [50,100,150,200,250,300,350,400,450,500,550,600,650,700]:\n",
    "    \n",
    "    obj = GaussianNB(features=training_data.iloc[:,0:1024],labels=training_data.iloc[:,1024],train_cv_test_split=(0.7,0.2,0.1),\n",
    "                             apply_pca=True,n_principal_components=n_components)\n",
    "\n",
    "    X_train = pd.DataFrame(obj.X_new)\n",
    "\n",
    "    X_train['label'] = training_data['label']\n",
    "\n",
    "    obj.fit(X_train,0.5,0.3)\n",
    "\n",
    "    X_cv = np.matmul(np.array(cv_data.iloc[:,0:1024]),obj.Q_tilda)\n",
    "\n",
    "    X_cv = pd.DataFrame(data=X_cv)\n",
    "\n",
    "    X_cv['label'] = cv_data['label']\n",
    "\n",
    "    D[n_components] = obj.evaluate(X_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{50: {'acc': 0.6053708439897698,\n",
       "  'recall': 0.6053708439897698,\n",
       "  'precision': 0.7089919101735292},\n",
       " 100: {'acc': 0.6829923273657289,\n",
       "  'recall': 0.6829923273657289,\n",
       "  'precision': 0.743782751047135},\n",
       " 150: {'acc': 0.7063938618925831,\n",
       "  'recall': 0.7063938618925831,\n",
       "  'precision': 0.7499621435097328},\n",
       " 200: {'acc': 0.718158567774936,\n",
       "  'recall': 0.718158567774936,\n",
       "  'precision': 0.7550919367418554},\n",
       " 250: {'acc': 0.7253196930946292,\n",
       "  'recall': 0.7253196930946292,\n",
       "  'precision': 0.7578255417191467},\n",
       " 300: {'acc': 0.7292838874680307,\n",
       "  'recall': 0.7292838874680307,\n",
       "  'precision': 0.7585437955792808},\n",
       " 350: {'acc': 0.7330562659846548,\n",
       "  'recall': 0.7330562659846548,\n",
       "  'precision': 0.7597412924528109},\n",
       " 400: {'acc': 0.7351023017902814,\n",
       "  'recall': 0.7351023017902814,\n",
       "  'precision': 0.7602376222470921},\n",
       " 450: {'acc': 0.7365728900255755,\n",
       "  'recall': 0.7365728900255755,\n",
       "  'precision': 0.7602746863769322},\n",
       " 500: {'acc': 0.7368286445012787,\n",
       "  'recall': 0.7368286445012787,\n",
       "  'precision': 0.7594875599276915},\n",
       " 550: {'acc': 0.7369565217391304,\n",
       "  'recall': 0.7369565217391304,\n",
       "  'precision': 0.7588942417893608},\n",
       " 600: {'acc': 0.7368925831202046,\n",
       "  'recall': 0.7368925831202046,\n",
       "  'precision': 0.7580233710554016},\n",
       " 650: {'acc': 0.7370843989769821,\n",
       "  'recall': 0.7370843989769821,\n",
       "  'precision': 0.7579194931482185},\n",
       " 700: {'acc': 0.7370204603580562,\n",
       "  'recall': 0.7370204603580562,\n",
       "  'precision': 0.7574623509487249}}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
